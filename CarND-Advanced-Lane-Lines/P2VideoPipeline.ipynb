{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project 2: **Advanced Lane Lines Finding on the Road** \n",
    "\n",
    "##  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        self.averageCnt = 1\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=2):\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\n",
    "\n",
    "            \n",
    "def abs_sobel_thresh(image, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Grayscaling input image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate directional gradient\n",
    "    if orient == \"x\":\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    elif orient == \"y\":\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Absolute values and normalizing\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Apply thresholds\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[np.logical_and(scaled_sobel >= thresh[0], scaled_sobel <= thresh[1])] = 1\n",
    "    return grad_binary\n",
    "\n",
    "\n",
    "def mag_thresh(image, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Grayscaling input image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate directional gradient\n",
    "    xsobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    ysobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    # Absolute values and normalizing\n",
    "    magn_sobel = np.sqrt(np.square(xsobel) + np.square(ysobel))\n",
    "    scaled_magn = np.uint8(255*magn_sobel/np.max(magn_sobel))\n",
    "    # Apply thresholds\n",
    "    mag_binary = np.zeros_like(magn_sobel)\n",
    "    mag_binary[np.logical_and(scaled_magn >= mag_thresh[0], scaled_magn <= mag_thresh[1])] = 1\n",
    "    return mag_binary\n",
    "\n",
    "\n",
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Grayscaling input image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate directional gradient\n",
    "    xsobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    ysobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_xsobel = np.absolute(xsobel)\n",
    "    abs_ysobel = np.absolute(ysobel)\n",
    "    # Gradient direction and normalization\n",
    "    direct_sobel = np.arctan2(abs_ysobel, abs_xsobel)\n",
    "    # Apply thresholds\n",
    "    dir_binary = np.zeros_like(direct_sobel)\n",
    "    dir_binary[np.logical_and(direct_sobel >= thresh[0], direct_sobel <= thresh[1])] = 1\n",
    "    return dir_binary\n",
    "\n",
    "\n",
    "def find_lane_pixels(binary_warped):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Identify the nonzero pixels in x and y within the window #\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    try:\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    except ValueError:\n",
    "        # Avoids an error if the above is not implemented fully\n",
    "        pass\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    return leftx, lefty, rightx, righty\n",
    "\n",
    "\n",
    "def search_around_poly(binary_warped, left_fit_prev, right_fit_prev):\n",
    "    # HYPERPARAMETER\n",
    "    # Choose the Width of the margin around the previous polynomial to search\n",
    "    margin = 100\n",
    "    # Grab activated pixels\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Search for lane points based on previous poloynomial\n",
    "    left_lane_inds = ((nonzerox > (left_fit_prev[0]*(nonzeroy**2) + left_fit_prev[1]*nonzeroy + \n",
    "                    left_fit_prev[2] - margin)) & (nonzerox < (left_fit_prev[0]*(nonzeroy**2) + \n",
    "                    left_fit_prev[1]*nonzeroy + left_fit_prev[2] + margin)))\n",
    "    right_lane_inds = ((nonzerox > (right_fit_prev[0]*(nonzeroy**2) + right_fit_prev[1]*nonzeroy + \n",
    "                    right_fit_prev[2] - margin)) & (nonzerox < (right_fit_prev[0]*(nonzeroy**2) + \n",
    "                    right_fit_prev[1]*nonzeroy + right_fit_prev[2] + margin)))\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    return leftx, lefty, rightx, righty\n",
    "    \n",
    "    \n",
    "def fit_polynomial(imgShape, leftx, lefty, rightx, righty):\n",
    "    # Fit a second order polynomial to each using `np.polyfit`\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, imgShape[0] - 1, imgShape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    return ploty, left_fitx, right_fitx, left_fit, right_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(leftLaneFindings, rightLaneFindings):\n",
    "    # Extract information \n",
    "    # Left lane\n",
    "    left_lane_points = leftLaneFindings[0]\n",
    "    left_lane_poly_coeffs = leftLaneFindings[2]\n",
    "    left_lane_curvature = leftLaneFindings[1]\n",
    "    left_lane_pixels = leftLaneFindings[3]\n",
    "    leftLaneValidity = True\n",
    "    # Right lane\n",
    "    right_lane_points = rightLaneFindings[0]\n",
    "    right_lane_poly_coeffs = rightLaneFindings[2]\n",
    "    right_lane_curvature = rightLaneFindings[1]\n",
    "    right_lane_pixels = rightLaneFindings[3]\n",
    "    rightLaneValidity = True\n",
    "    \n",
    "    # Check whether this is the first valid frame\n",
    "    # Due to calulating average information, there has to be at least one valid frame\n",
    "    firstFrameLeft = True\n",
    "    firstFrameRight = True\n",
    "    # Left lane\n",
    "    if leftLane.bestx is not None:\n",
    "        firstFrameLeft = False\n",
    "    # Right Lane\n",
    "    if rightLane.bestx is not None: \n",
    "        firstFrameRight = False\n",
    "      \n",
    "    # Check whether more than 50 lane points have been identified \n",
    "    # This determines whether we use (entire) sliding window approach for the upcoming frame\n",
    "    # Left Lane\n",
    "    if len(left_lane_pixels[0]) > 1000:\n",
    "        leftLane.detected = True\n",
    "    # Not enough left lane points available    \n",
    "    else:\n",
    "        leftLane.detected = False\n",
    "    # Right Lane\n",
    "    if len(right_lane_pixels[0]) > 1000:\n",
    "        rightLane.detected = True\n",
    "    else: \n",
    "        rightLane.detected = False\n",
    "\n",
    "    # Ceck Validity of lane findings\n",
    "    # Left Lane\n",
    "    if firstFrameLeft is not True: \n",
    "        # Check whether curvature changed too much compared to previous value\n",
    "        if ((abs(leftLane.current_fit[0] - left_lane_poly_coeffs[0]) > 7e-4) or  \n",
    "             (abs(leftLane.current_fit[1] - left_lane_poly_coeffs[1]) > 5) or \n",
    "             (abs(leftLane.current_fit[2] - left_lane_poly_coeffs[2]) > 300)):  \n",
    "                leftLaneValidity = False\n",
    "        # Check curvature value\n",
    "        if (abs(left_lane_poly_coeffs[0]) > 7e-4):\n",
    "            leftLaneValidity = False\n",
    "    # Right Lane\n",
    "    if firstFrameRight is not True:\n",
    "        # Check whether curvature changed too much compared to previous value\n",
    "        if ((abs(rightLane.best_fit[0] - right_lane_poly_coeffs[0]) > 7e-4) or  \n",
    "           (abs(rightLane.best_fit[1] - right_lane_poly_coeffs[1]) > 5) or \n",
    "           (abs(rightLane.best_fit[2] - right_lane_poly_coeffs[2]) > 300)):  \n",
    "                rightLaneValidity = False\n",
    "        # Check curvature value\n",
    "        if (abs(right_lane_poly_coeffs[0]) > 7e-4):\n",
    "            leftLaneValidity = False\n",
    "    \n",
    "    # If findings make sense and there were enough lane pixels identified, \n",
    "    # update the respective lane class with current information\n",
    "    # Otherwise, use averaged information\n",
    "    # Left Lane\n",
    "    if leftLaneValidity == True and leftLane.detected == True: \n",
    "        leftLane.averageCnt += 1\n",
    "        leftLane.recent_xfitted = left_lane_points\n",
    "        leftLane.current_fit = left_lane_poly_coeffs\n",
    "        leftLane.radius_of_curvature = left_lane_curvature\n",
    "        leftLane.line_base_pos = imageShape[0]/2*(3.7/700) - left_lane_points[-1]*(3.7/700) \n",
    "        leftLane.allx = left_lane_pixels[0]\n",
    "        leftLane.ally = left_lane_pixels[1]\n",
    "        if firstFrameLeft == False:\n",
    "            leftLane.best_fit = np.mean( np.array([leftLane.best_fit, left_lane_poly_coeffs]), axis=0 )\n",
    "        else: \n",
    "            leftLane.best_fit = leftLane.current_fit    \n",
    "    # Right Lane    \n",
    "    if rightLaneValidity == True and rightLane.detected == True:\n",
    "        rightLane.averageCnt += 1\n",
    "        rightLane.recent_xfitted = right_lane_points\n",
    "        rightLane.current_fit = right_lane_poly_coeffs\n",
    "        rightLane.radius_of_curvature = right_lane_curvature\n",
    "        rightLane.line_base_pos = imageShape[0]/2*(3.7/700) - right_lane_points[-1]*(3.7/700) \n",
    "        rightLane.allx = right_lane_pixels[0]\n",
    "        rightLane.ally = right_lane_pixels[1]\n",
    "        if firstFrameRight == False: \n",
    "            rightLane.best_fit = np.mean( np.array([rightLane.best_fit, right_lane_poly_coeffs]), axis=0 )\n",
    "        else: \n",
    "            rightLane.best_fit = rightLane.current_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane Finding Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Camera calibration\n",
    "\n",
    "Camera calibartion is not actually an iterative step of the lane line finding pipeline but rather a preprocessing step. The calibration matrix and distortion coefficients are calculated based on the chessboard calibration data (i.e. images) once. Afterwards they are used as static values (data structures) in order to undistort input images when they are passed into the pipeline. The theoretical background behind camera calibration is to learn a camera model which transforms 3D real world points into the 2D camera space. If this transformation model (including distrubances) is known, one can apply the revere transformation in order to undistort the corresponding camera images, i.e. to filter out the distortions. Finding the camera model is done by solving an optimization problem. One aims to find the camera parameters which describes the transformation between known 3D world points and 2D images points best.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrateCamera(): \n",
    "    # Prepare object points and create data structure to store both object points and input points\n",
    "    # Object points represent the 3D point in the world cos\n",
    "    # Input points represent the 2D point in the image cos\n",
    "    # --> 1.) Define number of corners in x and y direction\n",
    "    nx = 9\n",
    "    ny = 6\n",
    "    # --> 2.) Manually define actual corner positions in the real world cos (Assumption: z=0 --> flat plane)\n",
    "    #         The structure is supposed to be like (x1, y1, z1) in an array format, i.e. [ [1, 1, 0], [2, 1, 0], ... ]\n",
    "    objp = np.zeros(((nx)*(ny),3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[1:nx+1,1:ny+1].T.reshape(-1,2)\n",
    "    # --> 3.) Define the image shape and (empty) lists for both object points and image points\n",
    "    imageShape = mpimg.imread(\"camera_cal/calibration1.jpg\").shape[1::-1]\n",
    "    objPoints = []\n",
    "    imgPoints = []\n",
    "    # Iterate over calibration images, search for corners and concatenate respective image points\"\n",
    "    calImgsList = os.listdir(\"camera_cal/\")\n",
    "    for imgName in calImgsList:\n",
    "        calImg = mpimg.imread(\"camera_cal/\" + imgName)\n",
    "        gray = cv2.cvtColor(calImg, cv2.COLOR_RGB2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "        if ret == True: \n",
    "            objPoints.append(objp)\n",
    "            imgPoints.append(corners)\n",
    "    # Calibrate the image points against the object points, i.e. calculate the camera model        \n",
    "    # --> The camera matrix and distortion coefficients can be used later in the pipeline \n",
    "    #     to undistort the camera image input(s)\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objPoints, imgPoints, imageShape, None, None)\n",
    "    return mtx, dist, imageShape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Distortion Correction\n",
    "Due to the fact that cameras adding some distortion, one has to componsate for these in order to use the actual (i.e. undistorted) image information within the pipeline. That´s why the first step in the lane line finding pipeline is to apply an undistortion filter utilizing the camera model (i.e. camera matrix) and the distortion coefficients. Both the camera model and the distortion coefficients has been acquired in the (offline) calibration step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistortImage(image, mtx, dist):\n",
    "    undistImg = cv2.undistort(image, mtx, dist, None, mtx)\n",
    "    return undistImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Color thresholds\n",
    "The next step in the pipeline is to use the undistroted image as input and to apply color thresholds which describe yellow and white lanes. Due to lighting and brightnes deviations it´s unfortunate to use RGB thresholding. A more robust solution is to utilize the S (saturation) channel in the HLS color space. The thresholds are manually adapted on the test images.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Gradient based thresholds\n",
    "Besided applying color thresholds, also gradient based thresholds are promising to identify lanes in the input images. Rather than utilizing knowledge about common colors of lanes, one uses gradient based information in order to laverage the contrast which lane markings have compared to the road. Also one can use the direction in which the corresponding color changes occurs. Like in the color threshold case, the parameters are manually adapted on the test images.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Combining both gradient and color based binary maps\n",
    "In order to merge the information from both spaces the color and gradient based binary maps are combined using logical OR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyColorGradientThresholds(image):\n",
    "    # Apply color threshold on the S channel in the HLS color space\n",
    "    # --> 1.) Transformation of the undistorted test image to the HLS space and separate the S channel \n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    S = hls[:,:,2]\n",
    "    # --> 2.) Define the color threshold and apply the threshold to the S-channel.\n",
    "    #         Results, i.e. whether a certain pixel is within (true) or out of (false) the threhold, are stored in a binary map\n",
    "    colorThresh = (100, 255)\n",
    "    colorBinary = np.zeros_like(S)\n",
    "    colorBinary[(S > colorThresh[0]) & (S <= colorThresh[1])] = 1 \n",
    "    # Apply gradient based thresholds and combine them in order to create a unique gradient based output\n",
    "    # --> 1.) Gradient magnitude\n",
    "    magBinary = mag_thresh(image, sobel_kernel=19, mag_thresh=(10, 30))\n",
    "    # --> 2.) Gradient in x and y direction\n",
    "    xGradBinary = abs_sobel_thresh(image, orient='x', sobel_kernel=11, thresh=(30, 100))\n",
    "    yGradBinary = abs_sobel_thresh(image, orient='y', sobel_kernel=19, thresh=(10, 50))\n",
    "    # --> 3.) Gradient direction\n",
    "    dirBinary = dir_threshold(image, sobel_kernel=17, thresh=(0.7, 1.3))\n",
    "    # --> 4.) Combine the above three findings into one single gradient based output\n",
    "    gradientCombBinary = np.zeros_like(magBinary)\n",
    "    gradientCombBinary[(xGradBinary==1) & (dirBinary==1)] = 1\n",
    "    # Combining color and gradient based filtered binary images\n",
    "    colorGradCombBinary = np.zeros_like(colorBinary)\n",
    "    colorGradCombBinary[(colorBinary==1) | (gradientCombBinary==1)] = 1\n",
    "    return colorGradCombBinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.) Perspective Transform\n",
    "One crucial part of lane finding is to identify which pixels/points belong to the left and to the right lanes. This task is easier to solve in an transformed perspective, where both lanes appear parallel (like it´s actually the case in the real world). The idea here is to eyeball where the left and right lane would appear in birds eye view. By choosing destination points which cover a rather big area of the image shape, one can \"zoom\" in a bit, which pretty means nothing by additionaly applying region of interest (ROI). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyBirdsEyeTransform(image):\n",
    "    # Unwarp image\n",
    "    leftBottom = (imageShape[1], 190) \n",
    "    rightBottom = (imageShape[1], 1120)\n",
    "    leftTop = (450, 595)\n",
    "    rightTop = (450, 690)\n",
    "    leftBottomTarget = (imageShape[1], 300)\n",
    "    rightBottomTarget = (imageShape[1], 1000)\n",
    "    leftTopTarget = (0, 300)\n",
    "    rightTopTarget = (0, 1000)\n",
    "    src = np.float32([[leftBottom[1], leftBottom[0]], \n",
    "                      [leftTop[1], leftTop[0]], \n",
    "                      [rightTop[1], rightTop[0]], \n",
    "                      [rightBottom[1], rightBottom[0]]])\n",
    "    dst = np.float32([[leftBottomTarget[1], leftBottomTarget[0]], \n",
    "                      [leftTopTarget[1], leftTopTarget[0]], \n",
    "                      [rightTopTarget[1], rightTopTarget[0]], \n",
    "                      [rightBottomTarget[1], rightBottomTarget[0]]])\n",
    "    # --> 2.) Calculate the perspective transormation matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # --> 3.) Apply the transformation matrix\n",
    "    warpedImg = cv2.warpPerspective(image, M, imageShape)\n",
    "    return warpedImg, M\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.) Pixel classification and Polynomial Model fitting \n",
    "In this step, we aim to find pixels which corresponds to lane lines. Further, the goal is to identify which pixels belong to the right and left lane. In a first step, one can construct a histogram which counts the pixels upwards in each single column (i.e. along the y-direction) This is done in the lower part of the image, assuming that the lane in fact straight there. The row (i.e the x-position) where the sum of all active (i.e. white) pixels has a max can be considered as lane center point. So, searching for two peaks in the histogram leads to the lane center position for both right and left lane. Using the lane center positions as initial setting, one can further apply a sliding window approach to track the lane along the entire picture.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLaneModel(image, left_fit_prev = [], right_fit_prev=[], slidingWindow = True):\n",
    "    # Classify and locate lane points (for left and right lane respectively)\n",
    "    # There are two options to search for lane points\n",
    "    # --> 1 Complete search using histogram and sliding window\n",
    "    # --> 2 Iterative search leveraging previous polyonomial model as region of interest \n",
    "    if slidingWindow == True: \n",
    "        leftx, lefty, rightx, righty = find_lane_pixels(image)\n",
    "    else: \n",
    "        leftx, lefty, rightx, righty = search_around_poly(image, left_fit_prev, right_fit_prev)\n",
    "    # Fit a polynomial model to the lane points\n",
    "    leftLanePoints = [leftx, lefty]\n",
    "    rightLanePoints = [rightx, righty]\n",
    "    ploty, left_fitx, right_fitx, left_fit_coeffs, right_fit_coeffs = fit_polynomial(imageShape, leftx, lefty, rightx, righty)\n",
    "    return ploty, left_fitx, right_fitx, left_fit_coeffs, right_fit_coeffs, leftLanePoints, rightLanePoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.) Calculating the curvature\n",
    "Given the polynomial lane models, one can calculate the corresponing radius curvature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_curvature_real(ploty, left_fit_cr, right_fit_cr):\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    # Define y-value where we want radius of curvature\n",
    "    # We'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    y_eval = np.max(ploty)\n",
    "    # Calculation of R_curve (radius of curvature)\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    return left_curverad, right_curverad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.) Car Lane Offset\n",
    "In real life, the car is not always perfectly driving exactly in the middle of the lane. That´s why the information how much the car is shifted might be valuable. Assuming that the camera is mounted in the middle of the car, one can further assume that the actual lane center point is located at the middle of the picture (in x direction). This means, if the car would drive perfectly in the middle of the lane, both lane lines are equally distanced from the image center point.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lane_offset(x_eval_left, x_eval_right):\n",
    "    # m/pixel \n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    # The x-middle of the image (in meter)\n",
    "    xImgCenterPoint = imageShape[0]/2 * xm_per_pix\n",
    "    # The actual lane center points (Acquired from the polynomial model)\n",
    "    leftLaneCenterPoint = x_eval_left*xm_per_pix\n",
    "    rightLaneCenterPoint = x_eval_right*xm_per_pix\n",
    "    centerPoint = (leftLaneCenterPoint + rightLaneCenterPoint)/2\n",
    "    # The deviation between lane center point and image center point measures how much the car shifted away from the lane center\n",
    "    # Right offset should be considered as positve shift\n",
    "    laneOffset = centerPoint - xImgCenterPoint\n",
    "    return laneOffset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.) Transforming results back to the camera (view) space\n",
    "Classification, lane matching and curvature calculation has been done in the birds eye view space. However, these information has to be projected back into the actual camera space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformBackToCameraView(image, originalImg, M, left_fitx, right_fitx, ploty, leftLanePixels, rightLanePixels):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(image).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "    # Invert the camera matrix\n",
    "    InverseM = np.linalg.inv(M)\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, InverseM, (imageShape[0], imageShape[1]))\n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(originalImg, 1, newwarp, 0.5, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderCurvatureAndLaneOffset(image, curvature, laneOffset): \n",
    "    text1 = \"Curvature: \" + str(curvature.round(2))\n",
    "    text2 = \"Lane Offset: \" + str(laneOffset.round(2)) + \"m\"\n",
    "    textList = [text1, text2]\n",
    "    for i in range(len(textList)):\n",
    "        cv2.putText(image, text=textList[i], org=(50,50*(i+1)), fontFace= cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    fontScale=1, color=(255,255,255), thickness=3, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lane class in order to keep track and filter lane data\n",
    "leftLane = Line()\n",
    "rightLane = Line()\n",
    "leftLane.detected = False\n",
    "rightLane.detected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run calibration in order to identify distortion coefficients and camera matrix\n",
    "cameraModel, distortionCoeffs, imageShape = calibrateCamera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct lane finding pipeline\n",
    "def process_image(image):\n",
    "    # 1 Apply color and gradient based thresholds\n",
    "    colorGradientFilteredImg = applyColorGradientThresholds(image)\n",
    "    # 2 Warp image into birds eye view\n",
    "    warpedImg, M = applyBirdsEyeTransform(colorGradientFilteredImg)\n",
    "    # 3 Extract the lane points and fit a polyonomial model \n",
    "    ploty, left_fitx, right_fitx, left_fit_coeffs, right_fit_coeffs, leftPts, rightPts = fitLaneModel(warpedImg)\n",
    "    # 4 Calculate curvature and lane offset\n",
    "    left_curv_real, right_curv_real = measure_curvature_real(ploty, left_fit_coeffs, right_fit_coeffs)\n",
    "    curvature = (left_curv_real + right_curv_real)/2\n",
    "    laneOffset = calculate_lane_offset(left_fitx[-1], right_fitx[-1])\n",
    "    # 5 Render lane area and back transformation to camera view space \n",
    "    lanesInImg = transformBackToCameraView(warpedImg, image, M, left_fitx, right_fitx, ploty, leftPts, rightPts)\n",
    "    # 6 Add curvature and lane center information\n",
    "    renderCurvatureAndLaneOffset(lanesInImg, curvature, laneOffset)\n",
    "    return lanesInImg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct lane finding pipeline\n",
    "def process_image_smooth(image):\n",
    "    # 1 Apply color and gradient based thresholds\n",
    "    colorGradientFilteredImg = applyColorGradientThresholds(image)\n",
    "    # 2 Warp image into birds eye view\n",
    "    warpedImg, M = applyBirdsEyeTransform(colorGradientFilteredImg)\n",
    "    # 3 Extract the lane points and fit a polyonomial model\n",
    "    if leftLane.detected == False and rightLane.detected == False: \n",
    "        slidingWindow = True\n",
    "    else:\n",
    "        slidingWindow = False\n",
    "    left_fit_prev = leftLane.current_fit\n",
    "    right_fit_prev = rightLane.current_fit\n",
    "    ploty, left_fitx, right_fitx, left_fit_coeffs, right_fit_coeffs, leftPts, rightPts = fitLaneModel(warpedImg, left_fit_prev, right_fit_prev, slidingWindow)\n",
    "    # 4 Calculate curvature and lane offset\n",
    "    left_curv_real, right_curv_real = measure_curvature_real(ploty, left_fit_coeffs, right_fit_coeffs)\n",
    "    curvature = (left_curv_real + right_curv_real)/2\n",
    "    laneOffset = calculate_lane_offset(left_fitx[-1], right_fitx[-1])\n",
    "    # 5 Run sanity check over findings in this particular image and update the lane class information accordingly\n",
    "    leftLaneFindings = [left_fitx, left_curv_real, left_fit_coeffs, leftPts]\n",
    "    rightLaneFindings = [right_fitx, right_curv_real, right_fit_coeffs, rightPts]\n",
    "    sanityCheck(leftLaneFindings, rightLaneFindings)\n",
    "    # 6 Render lane area and back transformation to camera view space \n",
    "    # Take lane information which is tracked in the lane classes\n",
    "    left_fitx = leftLane.recent_xfitted \n",
    "    right_fitx = rightLane.recent_xfitted\n",
    "    leftPts = leftLane.allx\n",
    "    rightPts = rightLane.allx\n",
    "    curvature = (leftLane.radius_of_curvature + rightLane.radius_of_curvature)/2\n",
    "    laneOffset = calculate_lane_offset(left_fitx[-1], right_fitx[-1])\n",
    "    lanesInImg = transformBackToCameraView(warpedImg, image, M, left_fitx, right_fitx, ploty, leftPts, rightPts)\n",
    "    # 7 Add curvature and lane center information\n",
    "    renderCurvatureAndLaneOffset(lanesInImg, curvature, laneOffset)\n",
    "    return lanesInImg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video laneFindingVideoOutput.mp4\n",
      "[MoviePy] Writing video laneFindingVideoOutput.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [04:48<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: laneFindingVideoOutput.mp4 \n",
      "\n",
      "Wall time: 4min 49s\n"
     ]
    }
   ],
   "source": [
    "video_output = 'laneFindingVideoOutput.mp4'\n",
    "clip = VideoFileClip(\"project_video.mp4\")\n",
    "project_clip = clip.fl_image(process_image)\n",
    "#project_clip = clip.fl_image(process_image_smooth) \n",
    "%time project_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
